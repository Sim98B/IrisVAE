{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca59bdf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:17:40.880211Z",
     "start_time": "2024-12-07T13:17:39.488947Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5834b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:17:41.371095Z",
     "start_time": "2024-12-07T13:17:41.339471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "print(f'Actual device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7647be8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:17:41.932597Z",
     "start_time": "2024-12-07T13:17:41.917283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
       "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
       "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
       "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = load_iris()['data']\n",
    "X\n",
    "min_max = MinMaxScaler()\n",
    "X = min_max.fit_transform(X)\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b083ac9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:17:42.722691Z",
     "start_time": "2024-12-07T13:17:42.713583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n",
      "tensor([[0.2222, 0.7083, 0.0847, 0.1250],\n",
      "        [0.9444, 0.4167, 0.8644, 0.9167],\n",
      "        [0.0833, 0.4583, 0.0847, 0.0417],\n",
      "        [0.6111, 0.4167, 0.8136, 0.8750],\n",
      "        [0.5000, 0.2500, 0.7797, 0.5417],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0833],\n",
      "        [0.2222, 0.7500, 0.0847, 0.0833],\n",
      "        [0.6111, 0.4167, 0.7119, 0.7917],\n",
      "        [0.5556, 0.3333, 0.6949, 0.5833],\n",
      "        [0.5000, 0.4167, 0.6610, 0.7083],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.4167, 0.3333, 0.6949, 0.9583],\n",
      "        [0.6667, 0.5417, 0.7966, 0.8333],\n",
      "        [0.1111, 0.5000, 0.1017, 0.0417],\n",
      "        [0.5278, 0.3333, 0.6441, 0.7083],\n",
      "        [0.0833, 0.6667, 0.0000, 0.0417],\n",
      "        [0.1667, 0.4583, 0.0847, 0.0000],\n",
      "        [0.5833, 0.5000, 0.5932, 0.5833],\n",
      "        [0.1944, 0.5833, 0.0847, 0.0417],\n",
      "        [0.8056, 0.6667, 0.8644, 1.0000],\n",
      "        [0.5833, 0.2917, 0.7288, 0.7500],\n",
      "        [0.8333, 0.3750, 0.8983, 0.7083],\n",
      "        [0.2778, 0.7083, 0.0847, 0.0417],\n",
      "        [0.6111, 0.3333, 0.6102, 0.5833],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.1944, 0.5417, 0.0678, 0.0417],\n",
      "        [0.1389, 0.4167, 0.0678, 0.0833],\n",
      "        [0.5556, 0.5833, 0.7797, 0.9583],\n",
      "        [0.0278, 0.3750, 0.0678, 0.0417],\n",
      "        [0.0000, 0.4167, 0.0169, 0.0000],\n",
      "        [0.1944, 0.6667, 0.0678, 0.0417]])\n"
     ]
    }
   ],
   "source": [
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype = torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = IrisDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size = 32, shuffle = True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(f'{batch.shape}\\n{batch}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fecc5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T14:15:21.646045Z",
     "start_time": "2024-12-07T14:15:21.641556Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in dataloader:\n",
    "    batch = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "370c1312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T14:22:40.688822Z",
     "start_time": "2024-12-07T14:22:40.659502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3392,  0.3685],\n",
       "         [-0.3392,  0.3685],\n",
       "         [-0.3144,  0.2893],\n",
       "         [-0.3393,  0.3689],\n",
       "         [-0.3144,  0.2893],\n",
       "         [-0.3401,  0.3701],\n",
       "         [-0.3151,  0.2904],\n",
       "         [-0.3144,  0.2893],\n",
       "         [-0.3153,  0.2907],\n",
       "         [-0.3394,  0.3692],\n",
       "         [-0.3153,  0.2907],\n",
       "         [-0.3396,  0.3692],\n",
       "         [-0.3151,  0.2904],\n",
       "         [-0.3144,  0.2893],\n",
       "         [-0.3153,  0.2907],\n",
       "         [-0.3402,  0.3704],\n",
       "         [-0.3392,  0.3688],\n",
       "         [-0.3401,  0.3704],\n",
       "         [-0.3392,  0.3685],\n",
       "         [-0.3401,  0.3700],\n",
       "         [-0.3397,  0.3695],\n",
       "         [-0.3144,  0.2893],\n",
       "         [-0.3144,  0.2893],\n",
       "         [-0.3144,  0.2893],\n",
       "         [-0.3394,  0.3692],\n",
       "         [-0.3148,  0.2898],\n",
       "         [-0.3153,  0.2907],\n",
       "         [-0.3391,  0.3684],\n",
       "         [-0.3403,  0.3701],\n",
       "         [-0.3392,  0.3685],\n",
       "         [-0.3144,  0.2893],\n",
       "         [-0.3144,  0.2893]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.0873, -0.1581],\n",
       "         [-0.0873, -0.1581],\n",
       "         [-0.0062, -0.1269],\n",
       "         [-0.0876, -0.1583],\n",
       "         [-0.0062, -0.1269],\n",
       "         [-0.0872, -0.1585],\n",
       "         [-0.0056, -0.1270],\n",
       "         [-0.0062, -0.1269],\n",
       "         [-0.0054, -0.1271],\n",
       "         [-0.0880, -0.1584],\n",
       "         [-0.0054, -0.1271],\n",
       "         [-0.0868, -0.1582],\n",
       "         [-0.0056, -0.1270],\n",
       "         [-0.0062, -0.1269],\n",
       "         [-0.0054, -0.1271],\n",
       "         [-0.0869, -0.1585],\n",
       "         [-0.0875, -0.1582],\n",
       "         [-0.0875, -0.1586],\n",
       "         [-0.0873, -0.1582],\n",
       "         [-0.0866, -0.1583],\n",
       "         [-0.0871, -0.1583],\n",
       "         [-0.0062, -0.1269],\n",
       "         [-0.0062, -0.1269],\n",
       "         [-0.0062, -0.1269],\n",
       "         [-0.0879, -0.1584],\n",
       "         [-0.0059, -0.1270],\n",
       "         [-0.0054, -0.1271],\n",
       "         [-0.0871, -0.1581],\n",
       "         [-0.0861, -0.1583],\n",
       "         [-0.0873, -0.1581],\n",
       "         [-0.0062, -0.1269],\n",
       "         [-0.0062, -0.1269]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-1.0522,  0.2087],\n",
       "         [-1.0521,  0.2086],\n",
       "         [-0.6357, -0.0502],\n",
       "         [-1.0539,  0.2098],\n",
       "         [-0.6357, -0.0502],\n",
       "         [-1.0546,  0.2121],\n",
       "         [-0.6355, -0.0485],\n",
       "         [-0.6357, -0.0502],\n",
       "         [-0.6354, -0.0479],\n",
       "         [-1.0558,  0.2109],\n",
       "         [-0.6354, -0.0479],\n",
       "         [-1.0517,  0.2097],\n",
       "         [-0.6355, -0.0484],\n",
       "         [-0.6357, -0.0502],\n",
       "         [-0.6355, -0.0479],\n",
       "         [-1.0540,  0.2123],\n",
       "         [-1.0534,  0.2094],\n",
       "         [-1.0560,  0.2130],\n",
       "         [-1.0522,  0.2087],\n",
       "         [-1.0522,  0.2112],\n",
       "         [-1.0530,  0.2105],\n",
       "         [-0.6357, -0.0502],\n",
       "         [-0.6357, -0.0502],\n",
       "         [-0.6357, -0.0502],\n",
       "         [-1.0555,  0.2107],\n",
       "         [-0.6356, -0.0493],\n",
       "         [-0.6354, -0.0479],\n",
       "         [-1.0514,  0.2082],\n",
       "         [-1.0508,  0.2109],\n",
       "         [-1.0522,  0.2087],\n",
       "         [-0.6357, -0.0502],\n",
       "         [-0.6357, -0.0502]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.9103, -0.2341],\n",
       "         [-1.1162,  0.6075],\n",
       "         [-0.7780,  0.2317],\n",
       "         [-0.3028,  0.0215],\n",
       "         [ 0.1093, -1.4919],\n",
       "         [ 0.2856,  1.1827],\n",
       "         [ 0.2862, -0.8268],\n",
       "         [-1.3621,  0.6823],\n",
       "         [ 1.7215,  0.8412],\n",
       "         [-0.4300,  2.1850],\n",
       "         [-1.7238,  1.3525],\n",
       "         [ 0.6382,  1.9136],\n",
       "         [-2.1528,  0.4481],\n",
       "         [-0.6994,  0.7348],\n",
       "         [-1.1715,  0.8826],\n",
       "         [-0.7098,  0.0376],\n",
       "         [ 0.0342, -1.4108],\n",
       "         [-1.3372,  1.6992],\n",
       "         [-0.0479, -0.0356],\n",
       "         [-2.3188,  0.9243],\n",
       "         [-0.9839,  1.7339],\n",
       "         [-1.1412,  1.0417],\n",
       "         [ 0.2146, -0.7872],\n",
       "         [ 0.4338,  0.2369],\n",
       "         [ 1.8693,  1.2895],\n",
       "         [ 0.5859, -0.4795],\n",
       "         [-0.1137,  1.3076],\n",
       "         [ 1.2851,  1.0061],\n",
       "         [ 1.6681,  0.5180],\n",
       "         [-0.1334,  0.7763],\n",
       "         [-1.6681, -0.5220],\n",
       "         [ 0.6699,  0.5991]], grad_fn=<AddBackward0>),\n",
       " tensor([[0.5193, 0.4721, 0.5528, 0.5599],\n",
       "         [0.5179, 0.4365, 0.5191, 0.5434],\n",
       "         [0.5156, 0.4446, 0.5236, 0.5534],\n",
       "         [0.5672, 0.4417, 0.5632, 0.5650],\n",
       "         [0.5404, 0.4323, 0.5245, 0.5322],\n",
       "         [0.5660, 0.4320, 0.5754, 0.6476],\n",
       "         [0.5179, 0.4793, 0.5185, 0.6017],\n",
       "         [0.5160, 0.4682, 0.5468, 0.5870],\n",
       "         [0.5518, 0.4362, 0.5849, 0.5814],\n",
       "         [0.5056, 0.4830, 0.5940, 0.6027],\n",
       "         [0.5308, 0.4500, 0.5526, 0.6234],\n",
       "         [0.5075, 0.4643, 0.6074, 0.6397],\n",
       "         [0.5769, 0.4236, 0.5245, 0.5418],\n",
       "         [0.5309, 0.4144, 0.5617, 0.5714],\n",
       "         [0.5350, 0.4439, 0.5388, 0.5508],\n",
       "         [0.4695, 0.4231, 0.5418, 0.5853],\n",
       "         [0.4766, 0.4816, 0.5519, 0.5599],\n",
       "         [0.5199, 0.4553, 0.5578, 0.5660],\n",
       "         [0.5057, 0.4067, 0.5255, 0.5577],\n",
       "         [0.4982, 0.4216, 0.5262, 0.5626],\n",
       "         [0.5522, 0.4316, 0.5377, 0.5732],\n",
       "         [0.5637, 0.3696, 0.4928, 0.5892],\n",
       "         [0.4821, 0.4513, 0.5710, 0.5650],\n",
       "         [0.5421, 0.4376, 0.5885, 0.6200],\n",
       "         [0.5411, 0.4964, 0.4621, 0.5006],\n",
       "         [0.5076, 0.4399, 0.5119, 0.5864],\n",
       "         [0.5313, 0.4411, 0.5290, 0.5473],\n",
       "         [0.4998, 0.4630, 0.5470, 0.5855],\n",
       "         [0.4993, 0.4352, 0.4916, 0.5636],\n",
       "         [0.4982, 0.4458, 0.5231, 0.5762],\n",
       "         [0.5443, 0.4583, 0.5513, 0.5352],\n",
       "         [0.5457, 0.4545, 0.5168, 0.5404]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim = 4, \n",
    "                 layers = 5,\n",
    "                 dropout_rate = 0.5,\n",
    "                 latent_space_dim = 2):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        \n",
    "        max_neurons = 2 ** layers\n",
    "\n",
    "        encoder_layers_list = [\n",
    "            nn.Linear(input_dim, max_neurons),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ]\n",
    "        \n",
    "        current_dim = max_neurons\n",
    "        while current_dim > latent_space_dim:\n",
    "            next_dim = current_dim // 2\n",
    "            encoder_layers_list.extend(nn.Sequential(\n",
    "                nn.Linear(current_dim, next_dim),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ))\n",
    "            current_dim = next_dim\n",
    "        \n",
    "        encoder_layers_list.append(nn.Linear(current_dim, latent_space_dim))\n",
    "\n",
    "        self.Encoder = nn.Sequential(*encoder_layers_list)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(next_dim, latent_space_dim)\n",
    "        self.fc_logvar = nn.Linear(next_dim, latent_space_dim)\n",
    "        \n",
    "        decoder_layers_list = []\n",
    "        current_dim = latent_space_dim\n",
    "        while current_dim < max_neurons:\n",
    "            next_dim = current_dim * 2\n",
    "            decoder_layers_list.extend([\n",
    "                nn.Linear(current_dim, next_dim),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            current_dim = next_dim\n",
    "        \n",
    "        # L'ultimo strato del decoder per ricostruire l'input\n",
    "        decoder_layers_list.append(nn.Linear(current_dim, input_dim))\n",
    "        decoder_layers_list.append(nn.Sigmoid())\n",
    "        \n",
    "        self.Decoder = nn.Sequential(*decoder_layers_list)\n",
    "        \n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Implementa il trick della rielaborazione per campionare dallo spazio latente.\n",
    "        Args:\n",
    "            mu (torch.Tensor): Media della distribuzione latente (dimensione: batch_size x latent_dim).\n",
    "            logvar (torch.Tensor): Log-varianza della distribuzione latente (dimensione: batch_size x latent_dim).\n",
    "        Returns:\n",
    "            z (torch.Tensor): Campione dallo spazio latente (dimensione: batch_size x latent_dim).\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)  # Calcola lo scarto quadratico medio\n",
    "        epsilon = torch.randn_like(std)  # Campiona da una distribuzione normale standard\n",
    "        z = mu + epsilon * std  # Applica il trick della rielaborazione\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        mu, log_var = self.fc_mu(x), self.fc_logvar(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstruction = self.Decoder(z)\n",
    "        \n",
    "        return mu, log_var, x, z, reconstruction\n",
    "        \n",
    "        \n",
    "vae = VariationalAutoEncoder()\n",
    "vae\n",
    "vae(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073b856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
